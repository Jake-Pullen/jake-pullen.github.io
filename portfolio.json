{
  "name": "Jake Pullen",
  "label": "Data Engineer",
  "image_path": "img/main_photo.jpg",
  "contact": {
    "email": "hello@jake-is.me",
    "location": "Kettering, Northampton, England"
  },
  "summary": [
    "Transformational Data Engineer: Driving Business Growth through Data-Driven Insights",
    "Results-driven professional with a proven track record of tackling complex data challenges across multiple industries.",
    "With a passion for harnessing the power of data to inform strategic decision-making, I deliver actionable insights that drive business growth and unlock hidden opportunities.",
    "Skilled in designing and implementing scalable data architectures, I excel at collaborating with cross-functional teams to bridge gaps between data science, business strategy, and technical execution."
  ],
  "base_url": "127.0.0.1:5500",
  "social_links": [
    {
      "label": "Jake - LinkedIn",
      "url": "https://www.linkedin.com/in/jake-pullen/",
      "svg_path": "img/linkedin.svg"
    },
    {
      "label": "Jake - GitHub",
      "url": "https://github.com/Jake-Pullen",
      "svg_path": "img/github.svg"
    }
  ],
  "work_experience": [
    {
      "company": "Aiimi",
      "position": "Consultant Data Engineer",
      "url": "https://www.aiimi.com/",
      "start_date": "May 2024",
      "end_date": "Present",
      "summary": "As a trusted data engineer, I leverage my expertise to design, develop, and optimize data systems that unlock valuable insights for businesses. My primary focus is on transforming raw data into actionable intelligence, driving informed decision-making and business growth. By delivering high-quality data solutions, I help clients stay ahead in their industry.",
      "highlights": [
        "I design, develop, and maintain scalable data products that integrate various data models and datasets, delivering actionable insights to drive business decisions.",
        "I developed a comprehensive data validation framework, empowering teams to create robust and repeatable tests, ensuring data quality and confidence in their data pipelines.",
        "I built an automated documentation tool that generates detailed documentation for complex data models, including markdown files and interactive mermaid diagrams for visualizing lineage, impact analysis, and entity relationships."
      ]
    },
    {
      "company": "The Access Group",
      "position": "Data Engineer",
      "url": "https://www.theaccessgroup.com/",
      "start_date": "April 2021",
      "end_date": "May 2024",
      "summary": "As the primary data engineer for finance in the healthcare sector, I played a critical role in driving financial reporting, forecasting, and invoice generation. Additionally, I led complex patient data migrations to support onboarding new clients and maintained the integrity of our data warehouse reporting layer.",
      "highlights": [
        "I developed and maintained a comprehensive financial forecasting tool that Integrated Care Boards (ICB) used to report to the NHS England",
        "I led patient data migration efforts for new clients, guaranteeing accurate and timely importation of vital patient information into our systems.",
        "I created a secure data deletion solution that ensured GDPR compliance by removing client data from our multi-client database. The application logged all deletions and operated within specified timeframes to balance data erasure with system performance.",
        "I enhanced the existing invoice generation system, significantly reducing error rates and incorporating advanced debugging capabilities for faster troubleshooting."
      ]
    },
    {
      "company": "CDW",
      "position": "Service Desk Analyst",
      "url": "https://www.uk.cdw.com/",
      "start_date": "July 2020",
      "end_date": "April 2021",
      "summary": "Serve as the primary point of contact for technical support issues across a diverse portfolio of clients. Manage and facilitate access to secure Data Centre facilities, ensuring efficient resolution of technical queries.",
      "highlights": [
        "Successfully interacted with second and third-line teams, on-site engineers, and end-users to provide timely and effective support, ensuring seamless resolution of technical issues and maintaining strong relationships with key stakeholders.",
        "Consistently met or exceeded Service Level Agreement (SLA) targets, demonstrating a commitment to delivering exceptional customer service while minimizing downtime and maximizing productivity for our clients.",
        "Effectively bridged the gap between technical and non-technical language, translating complex technical concepts into clear, concise explanations that end-users could understand, ensuring effective support and resolution of issues in a rapidly changing technical environment."
      ]
    }
  ],
  "projects": [
    {
      "title": "Data Pipeline for YNAB",
      "summary": "I designed and developed a robust data ingestion application, adhering to industry best practices in ETL and Medallion architecture, with a focus on seamless scalability, reliability, and maintainability. The application features comprehensive error handling, logging, unit testing, and proper exit codes, ensuring fast and reliable debugging capabilities, all while leveraging optimal tools such as Polars, YAML, and Parquet files.",
      "url": "https://github.com/Jake-Pullen/data_pipeline_for_YNAB/",
      "highlights": [
        "Improved data handling: This application effectively detects and resolves duplicated data and lost data, reducing unnecessary data transfer and improving load times through its delta-style approach.",
        "Accelerated processing: leveraging Rust-built Polars for rapid data processing, and minimizing file sizes with Parquet storage for efficient data transfer.",
        "Enhanced transparency: integrated documentation empowers users to quickly understand and debug the application's inner workings, while self-documenting code ensures ease of maintenance and updates."
      ]
    },
    {
      "title": "Advent of Code",
      "summary": "Embracing the Advent of Code challenge as an opportunity for growth, I dedicate myself to annually tackle this coding puzzle contest. Through every iteration, I strive to enhance my problem-solving skills, expand my technical knowledge, and cultivate a deeper understanding of computer science concepts.",
      "url": "https://github.com/Jake-Pullen/advent_of_code",
      "highlights": [
        "Developed expertise in Python, leveraging its strengths for efficient coding and problem-solving.",
        "Refined my ability to break down complex problems into manageable parts, and learned to adapt to unexpected inputs and edge cases.",
        "Improved code organization, readability, and maintainability through careful planning, commenting, and testing.",
        "Gained a deeper understanding of fundamental algorithms and data structures, including sorting, searching, graph traversal, and dynamic programming techniques.",
        "Developed skills in coding best practices, debugging, and version control (Git), ensuring reliable and efficient code deployment.",
        "Demonstrated proficiency with various data structures (e.g., arrays, lists, dictionaries) and their applications in solving computational problems.",
        "Revisited and reinforced core computer science concepts, including data types, object-oriented programming, recursion, and functional programming."
      ]
    }
  ],
  "Certificates": [
    {
      "institution": "Microsoft",
      "Certificates": [
        "Microsoft Data Engineer Associate (DP203)",
        "Microsoft Power BI Data Analyst Associate (PL300)"
      ]
    },
    {
      "institution": "DataBricks",
      "Certificates": [
        "Databricks Certified Data Engineer Associate (currently studying)"
      ]
    }
  ],
  "skills": [
    {
      "name": "Python, SQL, PySpark, Databases",
      "proficiency": 100,
      "proficiency_label": "Expert"
    },
    {
      "name": "Azure Cloud infrastructure, Azure DevOps, GIT, Azure Data Factory, AI Engineering, Linux",
      "proficiency": 80,
      "proficiency_label": "Advanced"
    },
    {
      "name": "NoSQL, DAX, Web Development (HTML, CSS, Flask, Authentication, APIs)",
      "proficiency": 60,
      "proficiency_label": "Clear understanding"
    }
  ],
  "interests": [
    {
      "name": "Coding",
      "summary": [
        "As a Data Engineer, I believe that continuous learning and improvement are essential to staying ahead in the industry. Outside of work, I dedicate time to coding as a hobby to maintain and expand my skills. By doing so, I'm able to keep my knowledge up-to-date with the latest trends, technologies, and best practices. This not only enhances my professional abilities but also enables me to tackle complex problems more efficiently and creatively.",
        "Moreover, coding as a hobby allows me to explore new areas of interest, experiment with innovative solutions, and develop a deeper understanding of software development principles. By doing so, I'm able to bring fresh perspectives and ideas back into the workplace, driving innovation and improvement in my projects."
      ]
    },
    {
      "name": "Logistical puzzle and simulation games",
      "summary": [
        "As a Data Engineer, I enjoy playing logistical puzzle and simulation games in my free time because they align with my skills and interests. These types of games challenge me to optimize routes, manage resources, and make data-driven decisions - all skills that are highly relevant to my work as a Data Engineer.",
        "Playing these games also helps me develop problem-solving skills under pressure, think critically, and analyze complex systems. Additionally, I'm able to apply mathematical concepts such as linear programming, dynamic programming, and graph theory to solve puzzles and optimize game outcomes.",
        "Moreover, these games provide an opportunity for me to work with large datasets, test different scenarios, and explore the impact of various factors on system performance. This hands-on experience helps me develop a deeper understanding of how data drives decision-making in complex systems - a valuable skillset for any Data Engineer."
      ]
    }
  ],
  "languages": [
    {
      "language": "English",
      "fluency": "Native speaker"
    },
    {
      "language": "German",
      "fluency": "A2 level of CEFR"
    }
  ]
}
